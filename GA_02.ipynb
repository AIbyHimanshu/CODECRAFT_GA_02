{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#1) Install Dependencies\n",
        "!pip -q install -U \\\n",
        "    \"diffusers==0.21.4\" \\\n",
        "    \"transformers==4.38.2\" \\\n",
        "    \"accelerate==0.27.2\" \\\n",
        "    \"safetensors==0.4.3\" \\\n",
        "    \"huggingface_hub==0.20.3\" \\\n",
        "    \"open_clip_torch==2.24.0\" \\\n",
        "    \"pillow==10.4.0\" \\\n",
        "    matplotlib tqdm\n",
        "\n",
        "print('All dependencies installed')"
      ],
      "metadata": {
        "id": "UjdMC9_gdyBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2) Imports & Helpers\n",
        "import os, time, warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import open_clip\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Device\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DTYPE  = torch.float16 if DEVICE == 'cuda' else torch.float32\n",
        "print(f'ðŸ”§ Device : {DEVICE}')\n",
        "if DEVICE == 'cuda':\n",
        "    print(f'   GPU    : {torch.cuda.get_device_name(0)}')\n",
        "    print(f'   VRAM   : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
        "\n",
        "# Helpers\n",
        "def ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "OUT_A = ensure_dir('outputs_model_A')\n",
        "OUT_B = ensure_dir('outputs_model_B')\n",
        "\n",
        "def save_pil_images(images, out_dir, prefix):\n",
        "    paths = []\n",
        "    for i, img in enumerate(images, 1):\n",
        "        p = os.path.join(out_dir, f'{prefix}_{i}.png')\n",
        "        img.save(p)\n",
        "        paths.append(p)\n",
        "    return paths\n",
        "\n",
        "print('Imports & helpers ready')"
      ],
      "metadata": {
        "id": "dZ1qvH3ZeD9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3) Define Prompts\n",
        "PROMPTS = [\n",
        "    'a futuristic cyberpunk street at night, neon lights, rain, ultra-detailed, cinematic',\n",
        "    'a robot barista serving coffee in a cozy cafe, warm lighting, cinematic',\n",
        "    'a floating castle above the clouds, sunrise, epic fantasy art, highly detailed',\n",
        "]\n",
        "\n",
        "NUM_IMAGES = 2    # images per prompt per model\n",
        "STEPS      = 28   # inference steps (tuned for T4)\n",
        "GUIDANCE   = 7.5  # classifier-free guidance scale\n",
        "\n",
        "print(f'{len(PROMPTS)} prompts Ã— {NUM_IMAGES} images Ã— 2 models = {len(PROMPTS)*NUM_IMAGES*2} total images')\n",
        "for i, p in enumerate(PROMPTS, 1):\n",
        "    print(f'  {i}. {p}')"
      ],
      "metadata": {
        "id": "rWW16iGPenQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4)  Load & Run Model A (SD v1.5)\n",
        "# â”€â”€ Model A: Stable Diffusion v1.5 (runwayml) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Same weights referenced in Ref #1 (TF/KerasCV SD Tutorial), PyTorch backend\n",
        "MODEL_A_ID    = 'runwayml/stable-diffusion-v1-5'\n",
        "MODEL_A_LABEL = 'SD v1.5 (runwayml)'\n",
        "\n",
        "print(f'Loading {MODEL_A_LABEL} â€¦')\n",
        "pipe_a = StableDiffusionPipeline.from_pretrained(\n",
        "    MODEL_A_ID,\n",
        "    torch_dtype=DTYPE,\n",
        "    safety_checker=None,\n",
        "    requires_safety_checker=False,\n",
        ").to(DEVICE)\n",
        "print(f'{MODEL_A_LABEL} loaded')\n",
        "\n",
        "results_A = {}\n",
        "times_A   = {}\n",
        "\n",
        "for idx, prompt in enumerate(PROMPTS, 1):\n",
        "    print(f'\\n[Model A] Prompt {idx}/{len(PROMPTS)}: \"{prompt[:60]}â€¦\"')\n",
        "    t0  = time.time()\n",
        "    out = pipe_a(\n",
        "        prompt,\n",
        "        num_images_per_prompt=NUM_IMAGES,\n",
        "        guidance_scale=GUIDANCE,\n",
        "        num_inference_steps=STEPS,\n",
        "        height=512, width=512,\n",
        "    )\n",
        "    dt = time.time() - t0\n",
        "    times_A[prompt]   = dt\n",
        "    results_A[prompt] = save_pil_images(out.images, OUT_A, f'prompt{idx}')\n",
        "    print(f' Done in {dt:.1f}s  â†’  saved {len(out.images)} images')\n",
        "\n",
        "# Free VRAM before loading Model B\n",
        "del pipe_a\n",
        "torch.cuda.empty_cache()\n",
        "print('\\nModel A complete. VRAM released.')"
      ],
      "metadata": {
        "id": "k58-z-kyiKEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ HuggingFace Login â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")  # paste your huggingface token here\n",
        "print(\"Logged in to HuggingFace\")"
      ],
      "metadata": {
        "id": "Ir03Gb9Po7b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5)  Load & Run Model B (SD v2.1)\n",
        "# â”€â”€ Model B: CompVis/stable-diffusion-v1-4 (100% public, no auth needed) â”€â”€â”€â”€â”€â”€\n",
        "# Different from Model A (v1.5): earlier checkpoint, different training mix,\n",
        "# good for comparison â€” same concept as DALL-E Mini (open generative model)\n",
        "\n",
        "MODEL_B_ID    = 'CompVis/stable-diffusion-v1-4'\n",
        "MODEL_B_LABEL = 'SD v1.4 (CompVis)'\n",
        "\n",
        "print(f'Loading {MODEL_B_LABEL} â€¦')\n",
        "pipe_b = StableDiffusionPipeline.from_pretrained(\n",
        "    MODEL_B_ID,\n",
        "    torch_dtype=DTYPE,\n",
        "    safety_checker=None,\n",
        "    requires_safety_checker=False,\n",
        ").to(DEVICE)\n",
        "print(f'{MODEL_B_LABEL} loaded')\n",
        "\n",
        "results_B = {}\n",
        "times_B   = {}\n",
        "\n",
        "for idx, prompt in enumerate(PROMPTS, 1):\n",
        "    print(f'\\n[Model B] Prompt {idx}/{len(PROMPTS)}: \"{prompt[:60]}â€¦\"')\n",
        "    t0  = time.time()\n",
        "    out = pipe_b(\n",
        "        prompt,\n",
        "        num_images_per_prompt=NUM_IMAGES,\n",
        "        guidance_scale=GUIDANCE,\n",
        "        num_inference_steps=STEPS,\n",
        "        height=512, width=512,\n",
        "    )\n",
        "    dt = time.time() - t0\n",
        "    times_B[prompt]   = dt\n",
        "    results_B[prompt] = save_pil_images(out.images, OUT_B, f'prompt{idx}')\n",
        "    print(f'   Done in {dt:.1f}s  â†’  saved {len(out.images)} images')\n",
        "\n",
        "del pipe_b\n",
        "torch.cuda.empty_cache()\n",
        "print(f'\\nModel B ({MODEL_B_LABEL}) complete. VRAM released.')"
      ],
      "metadata": {
        "id": "0TfO2fAViOhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6) Visual Comparison Grids\n",
        "COLOR_A = '#7B5EA7'\n",
        "COLOR_B = '#E8B84B'\n",
        "\n",
        "def show_comparison(prompt, paths_a, paths_b):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 11))\n",
        "    fig.patch.set_facecolor('#0f0f1a')\n",
        "\n",
        "    pairs = [\n",
        "        (paths_a[0], 'SD v1.5  Â·  Image 1', COLOR_A),\n",
        "        (paths_b[0], 'SD v2.1  Â·  Image 1', COLOR_B),\n",
        "        (paths_a[1], 'SD v1.5  Â·  Image 2', COLOR_A),\n",
        "        (paths_b[1], 'SD v2.1  Â·  Image 2', COLOR_B),\n",
        "    ]\n",
        "\n",
        "    for ax, (path, title, color) in zip(axes.flat, pairs):\n",
        "        ax.imshow(np.array(Image.open(path)))\n",
        "        ax.axis('off')\n",
        "        ax.set_title(title, color=color, fontsize=11, fontweight='bold', pad=8)\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_edgecolor(color)\n",
        "            spine.set_linewidth(2)\n",
        "            spine.set_visible(True)\n",
        "\n",
        "    short = prompt if len(prompt) < 70 else prompt[:67] + 'â€¦'\n",
        "    fig.suptitle(f' {short}', color='white', fontsize=12, y=1.01, style='italic')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for prompt in PROMPTS:\n",
        "    show_comparison(prompt, results_A[prompt], results_B[prompt])"
      ],
      "metadata": {
        "id": "WNQtRzwwie4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7) CLIP Similarity Scoring\n",
        "print('Loading CLIP ViT-B/32 â€¦')\n",
        "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
        "    'ViT-B-32', pretrained='openai'\n",
        ")\n",
        "clip_model = clip_model.to(DEVICE).eval()\n",
        "clip_tok   = open_clip.get_tokenizer('ViT-B-32')\n",
        "print('CLIP ready')\n",
        "\n",
        "@torch.no_grad()\n",
        "def clip_score(prompt, image_path):\n",
        "    img    = clip_preprocess(Image.open(image_path).convert('RGB')).unsqueeze(0).to(DEVICE)\n",
        "    txt    = clip_tok([prompt]).to(DEVICE)\n",
        "    i_feat = clip_model.encode_image(img)\n",
        "    t_feat = clip_model.encode_text(txt)\n",
        "    i_feat /= i_feat.norm(dim=-1, keepdim=True)\n",
        "    t_feat /= t_feat.norm(dim=-1, keepdim=True)\n",
        "    return (i_feat * t_feat).sum().item()\n",
        "\n",
        "clip_scores_A = {}\n",
        "clip_scores_B = {}\n",
        "\n",
        "for prompt in PROMPTS:\n",
        "    clip_scores_A[prompt] = [clip_score(prompt, p) for p in results_A[prompt]]\n",
        "    clip_scores_B[prompt] = [clip_score(prompt, p) for p in results_B[prompt]]\n",
        "    print(f'\\nPrompt: \"{prompt[:55]}â€¦\"')\n",
        "    print(f'  SD v1.5 scores : {[f\"{s:.4f}\" for s in clip_scores_A[prompt]]}  avg={np.mean(clip_scores_A[prompt]):.4f}')\n",
        "    print(f'  SD v2.1 scores : {[f\"{s:.4f}\" for s in clip_scores_B[prompt]]}  avg={np.mean(clip_scores_B[prompt]):.4f}')"
      ],
      "metadata": {
        "id": "L69bRKB3jF4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8)  Results Tables\n",
        "def print_table(rows, headers):\n",
        "    widths = [len(h) for h in headers]\n",
        "    for r in rows:\n",
        "        for i, c in enumerate(r):\n",
        "            widths[i] = max(widths[i], len(str(c)))\n",
        "    sep = '-+-'.join('-' * w for w in widths)\n",
        "    print(' | '.join(str(h).ljust(widths[i]) for i, h in enumerate(headers)))\n",
        "    print(sep)\n",
        "    for r in rows:\n",
        "        print(' | '.join(str(c).ljust(widths[i]) for i, c in enumerate(r)))\n",
        "\n",
        "# Detailed table\n",
        "print('\\n DETAILED CLIP SCORES\\n')\n",
        "detail_rows = []\n",
        "for prompt in PROMPTS:\n",
        "    short = prompt[:48] + 'â€¦' if len(prompt) > 48 else prompt\n",
        "    for i, s in enumerate(clip_scores_A[prompt], 1):\n",
        "        detail_rows.append([short, 'SD v1.5', i, f'{s:.4f}', f'{times_A[prompt]:.1f}s'])\n",
        "    for i, s in enumerate(clip_scores_B[prompt], 1):\n",
        "        detail_rows.append([short, 'SD v2.1', i, f'{s:.4f}', f'{times_B[prompt]:.1f}s'])\n",
        "print_table(detail_rows, ['Prompt', 'Model', 'Img#', 'CLIP Score', 'Time'])\n",
        "\n",
        "# Summary table\n",
        "print('\\n\\n SUMMARY\\n')\n",
        "summary_rows = []\n",
        "for prompt in PROMPTS:\n",
        "    short  = prompt[:48] + 'â€¦' if len(prompt) > 48 else prompt\n",
        "    avg_a  = np.mean(clip_scores_A[prompt])\n",
        "    avg_b  = np.mean(clip_scores_B[prompt])\n",
        "    winner = 'SD v1.5 âœ“' if avg_a >= avg_b else 'SD v2.1 âœ“'\n",
        "    summary_rows.append([short, f'{avg_a:.4f}', f'{times_A[prompt]:.1f}s',\n",
        "                                 f'{avg_b:.4f}', f'{times_B[prompt]:.1f}s', winner])\n",
        "print_table(summary_rows, ['Prompt', 'A AvgCLIP', 'A Time', 'B AvgCLIP', 'B Time', 'Winner'])"
      ],
      "metadata": {
        "id": "CerGzXMvjJK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9) Bar Charts\n",
        "labels      = [f'P{i+1}' for i in range(len(PROMPTS))]\n",
        "avgs_a      = [np.mean(clip_scores_A[p]) for p in PROMPTS]\n",
        "avgs_b      = [np.mean(clip_scores_B[p]) for p in PROMPTS]\n",
        "times_a_lst = [times_A[p] for p in PROMPTS]\n",
        "times_b_lst = [times_B[p] for p in PROMPTS]\n",
        "\n",
        "x, w = np.arange(len(labels)), 0.35\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "fig.patch.set_facecolor('#0f0f1a')\n",
        "\n",
        "for ax in (ax1, ax2):\n",
        "    ax.set_facecolor('#16162a')\n",
        "    ax.tick_params(colors='white')\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_color('#333355')\n",
        "\n",
        "# CLIP chart\n",
        "b1 = ax1.bar(x - w/2, avgs_a, w, label='SD v1.5', color='#7B5EA7', alpha=0.9)\n",
        "b2 = ax1.bar(x + w/2, avgs_b, w, label='SD v2.1', color='#E8B84B', alpha=0.9)\n",
        "ax1.set_xticks(x); ax1.set_xticklabels(labels, color='white')\n",
        "ax1.set_ylabel('Avg CLIP Score', color='white')\n",
        "ax1.set_title('Promptâ€“Image Alignment (CLIP)', color='white', fontweight='bold')\n",
        "ax1.legend(facecolor='#16162a', labelcolor='white')\n",
        "ax1.bar_label(b1, fmt='%.3f', color='#7B5EA7', fontsize=9)\n",
        "ax1.bar_label(b2, fmt='%.3f', color='#E8B84B', fontsize=9)\n",
        "\n",
        "# Runtime chart\n",
        "b3 = ax2.bar(x - w/2, times_a_lst, w, label='SD v1.5 (512px)', color='#7B5EA7', alpha=0.9)\n",
        "b4 = ax2.bar(x + w/2, times_b_lst, w, label='SD v2.1 (768px)', color='#E8B84B', alpha=0.9)\n",
        "ax2.set_xticks(x); ax2.set_xticklabels(labels, color='white')\n",
        "ax2.set_ylabel('Generation Time (s)', color='white')\n",
        "ax2.set_title('Runtime per Prompt', color='white', fontweight='bold')\n",
        "ax2.legend(facecolor='#16162a', labelcolor='white')\n",
        "ax2.bar_label(b3, fmt='%.1fs', color='#7B5EA7', fontsize=9)\n",
        "ax2.bar_label(b4, fmt='%.1fs', color='#E8B84B', fontsize=9)\n",
        "\n",
        "plt.suptitle('Task 02 â€” Model Comparison: SD v1.5 vs SD v2.1',\n",
        "             color='white', fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('task02_comparison_chart.png', dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
        "plt.show()\n",
        "print('Chart saved â†’ task02_comparison_chart.png')"
      ],
      "metadata": {
        "id": "xlMS1tL7jOiY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}